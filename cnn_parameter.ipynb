{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "cnn_parameter.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bennsamuel/Pneumonia-Project/blob/master/cnn_parameter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1HJrduozTKM",
        "colab_type": "text"
      },
      "source": [
        "# Deletes log folder\n",
        "should not be used. Was implemented when cnn algorithm was not finshed to mingle around.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uukp28eezLbM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# %cd /content/drive/My Drive/Group 77 - Pneumonia/logs\n",
        "# !rm -d -r *\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khtTZm2LqT9b",
        "colab_type": "text"
      },
      "source": [
        "# START HERE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gCSpATnMr5e",
        "colab_type": "code",
        "outputId": "6505057e-2663-4ab1-bd3b-0228715cfaf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# START HERE\n",
        "\n",
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4AOCheJNu3U",
        "colab_type": "code",
        "outputId": "65371a56-0c51-44f9-e9aa-88ee717cc970",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# After executing the cell above, Drive\n",
        "# files will be present in \"/content/drive/My Drive\".\n",
        "!ls \"/content/drive/My Drive/Group 77 - Pneumonia/pickle/\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_data.pickle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peGASGqf1tHI",
        "colab_type": "code",
        "outputId": "f0d0e328-c8df-49d9-b4db-a566307f3704",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "IMG_SIZE = 224\n",
        "NUM_CLASSES = 3\n",
        "\n",
        "#                0          1         2\n",
        "CATEGORIES = [\"NORMAL\",\"BACTERIA\", \"VIRUS\"]\n",
        "\n",
        "\n",
        "%cd /content/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G534nMYhAJEY",
        "colab_type": "text"
      },
      "source": [
        "## `DATA SET which should be used for training is defined here.`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wHYgnMIKEPlo",
        "colab_type": "code",
        "outputId": "4cb41671-9dd4-4264-9c96-3457b887d7a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def parameter_study(WIDTH_SHIFT, HEIGHT_SHIFT, ZOOM, ROTATION, MIRROR_V, MIRROR_H):\n",
        "\n",
        "    data_set_path = \"/content/drive/My Drive/Group 77 - Pneumonia/pickle_augmented/aug-{}_{}_{}_{}_{}.pickle\".format(WIDTH_SHIFT,HEIGHT_SHIFT,ZOOM,ROTATION,MIRROR_V)\n",
        "    # extract name from dataset\n",
        "    specific_data_set = data_set_path.split('/')[-1]\n",
        "\n",
        "    # train set\n",
        "    data = pickle.load(open(data_set_path, \"rb\"))\n",
        "\n",
        "    # validation set\n",
        "    test_data = pickle.load(open(\"/content/drive/My Drive/Group 77 - Pneumonia/pickle/test_data.pickle\", \"rb\"))\n",
        "\n",
        "    print(\"Mounted data set: {}\".format(specific_data_set))\n",
        "\n",
        "    # splitting data in training data and validation data \n",
        "    # creating normalized SHUFFLED arrays\n",
        "    # DUE TO REFACTORING validaiton set it called test set\n",
        "    \n",
        "\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    for features, label in data:\n",
        "        X.append(features)\n",
        "        y.append(label)\n",
        "        \n",
        "    X_train = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "    y_train = y\n",
        "\n",
        "    X_test = []\n",
        "    y_test = []\n",
        "\n",
        "    for features, label in test_data:\n",
        "        X_test.append(features)\n",
        "        y_test.append(label)\n",
        "        \n",
        "    X_test = np.array(X_test).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
        "\n",
        "    X_train = np.asarray(X_train)\n",
        "    y_train = np.asarray(y_train)\n",
        "\n",
        "    # X_val = np.asarray(X_val)\n",
        "    # y_val = np.asarray(y_val)\n",
        "\n",
        "    X_test = np.asarray(X_test)\n",
        "    y_test = np.asarray(y_test)\n",
        "\n",
        "    X_train = X_train/255.0\n",
        "    # X_val = X_val/255.0\n",
        "    X_test = X_test/255.0\n",
        "\n",
        "\n",
        "\n",
        "    y_train = to_categorical(y_train, NUM_CLASSES)\n",
        "    y_test = to_categorical(y_test, NUM_CLASSES)\n",
        "\n",
        "\n",
        "    y_train.shape\n",
        "\n",
        "    NAME = \"{data_set} - {time}\".format(data_set = specific_data_set, time = time.strftime(\"%Y-%m-%d_%H:%M:%S\"))\n",
        "\n",
        "    tensorboard = TensorBoard(log_dir = \"/content/drive/My Drive/Group 77 - Pneumonia/logs/{}\".format(NAME))\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(64, kernel_size=(3, 3),\n",
        "                    strides=2,\n",
        "                    activation='relu',\n",
        "                    input_shape=X_train.shape[1:]))\n",
        "    model.add(Conv2D(64, kernel_size=(3, 3), strides=2, activation='relu'))\n",
        "    # model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(128, kernel_size=(3, 3), strides=2, activation='relu'))\n",
        "    model.add(Conv2D(128, kernel_size=(3, 3), strides=2, activation='relu'))\n",
        "    # model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "    model.add(Conv2D(256, kernel_size=(3, 3), strides=2, activation='relu'))\n",
        "    model.add(Conv2D(256, kernel_size=(3, 3), strides=2, activation='relu'))\n",
        "    # model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # w/o regularization\n",
        "    # model.add(Dense(128, activation='relu'))\n",
        "    # model.add(Dropout(0.5))\n",
        "\n",
        "    # model.add(Dense(128, activation='relu'))\n",
        "    # model.add(Dropout(0.5))\n",
        "\n",
        "    # with regularization\n",
        "    model.add(Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l=0.1)))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l=0.1)))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    # Dense 3 softmax because we have 3 classes\n",
        "    model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
        "\n",
        "    print(\"Mounted data set: {}\".format(specific_data_set))\n",
        "\n",
        "    model.layers\n",
        "\n",
        "    precision = tf.keras.metrics.Precision()\n",
        "    recall = tf.keras.metrics.Recall()\n",
        "\n",
        "    model.compile(loss=\"categorical_crossentropy\", \n",
        "                  optimizer=\"adam\", \n",
        "                  metrics=[\"accuracy\", precision, recall])\n",
        "\n",
        "    model.fit(X_train, y_train, batch_size=128, epochs=30, validation_data=(X_test,y_test), callbacks = [tensorboard])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted data set: test_data.pickle\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEsSqaMEWCpD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "for width_shift in range(0, 11, 5):\n",
        "    for height_shift in range(0, 11, 5):\n",
        "      for zoom in range(0, 22, 10):\n",
        "        for rotation in range(0, 22, 10):\n",
        "          for mirror_v in range(2):\n",
        "            mir_v = False if mirror_v == 0 else True\n",
        "            parameter_study(width_shift, height_shift, float(zoom)/100, rotation, mir_v)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}